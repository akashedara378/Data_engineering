Tuing:

1️⃣ Use DataFrame API Instead of RDD (DataFrames are optimized with Catalyst and Tungsten).
2️⃣ Enable Predicate Pushdown (Filter data early to reduce I/O).
3️⃣ Broadcast Small Tables in Joins (Avoid shuffle joins using broadcast()).
4️⃣ Optimize Shuffling

Increase partitions for large datasets (repartition()).
Reduce partitions for smaller datasets (coalesce()).
5️⃣ Cache & Persist Frequently Used Data (df.cache() or df.persist()).
6️⃣ Use Columnar File Formats (Prefer Parquet/ORC over CSV for better compression & performance).
7️⃣ Avoid UDFs When Possible (Use built-in Spark functions for better optimization).
8️⃣ Tune Spark Configurations
Adjust spark.sql.shuffle.partitions.
Optimize executor memory (--executor-memory & --executor-cores).


OOM:
1️⃣ Increase Executor & Driver Memory
Set higher memory limits:
--executor-memory 8G --driver-memory 4G
2️⃣ Optimize Data Partitioning
Increase partitions for large data (df.repartition(200)).
Reduce partitions before writing output (df.coalesce(10)).
3️⃣ Use DataFrame API Instead of RDDs (DataFrames are optimized with Catalyst and Tungsten).
4️⃣ Persist/Clear Intermediate Data
Use df.persist() to cache frequently used data.
Use df.unpersist() to free up memory when done.
5️⃣ Use Columnar File Formats (Parquet/ORC) (Reduces memory usage vs. CSV/JSON).
6️⃣ Avoid Collecting Large Data to Driver
❌ Avoid df.collect().
✅ Use .show(), .take(n), or .limit(n).


Delta lake:
SELECT * FROM sales_data VERSION AS OF 5;

OPTIMIZE sales_data ZORDER BY (region, order_date);

VACUUM sales_data;(delete older than 7 days)
VACUUM sales_data RETAIN 24 HOURS;
VACUUM sales_data RETAIN 24 HOURS DRY RUN
