Tuing:

1️⃣ Use DataFrame API Instead of RDD (DataFrames are optimized with Catalyst and Tungsten).
2️⃣ Enable Predicate Pushdown (Filter data early to reduce I/O).
3️⃣ Broadcast Small Tables in Joins (Avoid shuffle joins using broadcast()).
4️⃣ Optimize Shuffling

Increase partitions for large datasets (repartition()).
Reduce partitions for smaller datasets (coalesce()).
5️⃣ Cache & Persist Frequently Used Data (df.cache() or df.persist()).
6️⃣ Use Columnar File Formats (Prefer Parquet/ORC over CSV for better compression & performance).
7️⃣ Avoid UDFs When Possible (Use built-in Spark functions for better optimization).
8️⃣ Tune Spark Configurations
Adjust spark.sql.shuffle.partitions.
Optimize executor memory (--executor-memory & --executor-cores).


OOM:
1️⃣ Increase Executor & Driver Memory
Set higher memory limits:
--executor-memory 8G --driver-memory 4G
2️⃣ Optimize Data Partitioning
Increase partitions for large data (df.repartition(200)).
Reduce partitions before writing output (df.coalesce(10)).
3️⃣ Use DataFrame API Instead of RDDs (DataFrames are optimized with Catalyst and Tungsten).
4️⃣ Persist/Clear Intermediate Data
Use df.persist() to cache frequently used data.
Use df.unpersist() to free up memory when done.
5️⃣ Use Columnar File Formats (Parquet/ORC) (Reduces memory usage vs. CSV/JSON).
6️⃣ Avoid Collecting Large Data to Driver
❌ Avoid df.collect().
✅ Use .show(), .take(n), or .limit(n).


Delta lake:
SELECT * FROM sales_data VERSION AS OF 5;

OPTIMIZE sales_data ZORDER BY (region, order_date);

VACUUM sales_data;(delete older than 7 days)
VACUUM sales_data RETAIN 24 HOURS;
VACUUM sales_data RETAIN 24 HOURS DRY RUN


Total Executors= Executor Cores/Total Cluster Cores
​Executor Memory=Total Available Memory/Total Executors


4️⃣ How Do You Ensure Data Consistency and Fault Tolerance in a Distributed ETL Pipeline?
✅ Key Techniques:
Idempotency: Ensure reprocessing does not duplicate data (e.g., use unique keys).
Checkpointing: Use checkpointLocation in Spark Streaming to track progress.
ACID Transactions: Use Delta Lake for atomic writes & schema evolution.
Retry Logic: Implement retries for transient failures (e.g., ADF retries on failure).

​5️⃣ How Do You Handle Late-Arriving Data in Spark Streaming?
✅ Solutions:
1️⃣ Watermarking
Define withWatermark("event_time", "10 minutes") to allow a grace period for late events.
2️⃣ Event-Time Processing
Use windowing functions (e.g., df.groupBy(window(col("timestamp"), "5 minutes"))).
3️⃣ Reprocessing Late Data
Store raw events in Delta Lake and reprocess late data periodically.
4️⃣ Joining Late Events with Previous Data
Use stateful aggregations with mapWithState() to update previously computed results.
