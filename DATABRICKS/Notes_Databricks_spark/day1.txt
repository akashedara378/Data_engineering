

WHAT IS DATABRICKS?
ACQUIRE --> ARRANGE --> PROCESS --> ANALYZE --> VISUALIZE
--> DATABRICKS IS A PaaS (PLATFORM AS A SERVICE)
--> SINGLE WINDOW PLATFORM TO IMPLIMENT ENTIRE BIG DATA ECOSYSTEM
--> WE CAN IMPLEMENT DATA ANALYTICS, DATA ENGINEERING, DATA SCIENCE (ML), ALSO SUPPORTS ai

DATABRICKS FEATURES
 1. PURELY CLOUD SUPPORTED: SUPPORTS MULTI CLOUD --> AWS, AZURE, GCP
 2. COMPUTATIONAL ENGINE: SPARK (STANDARD/ML), SERVERLESS (SQL WAREHOUSE)
 3. SUPPORTS LARGE DATA
 4. DBFS : DATABRICKS FILE SYSTEM
 5. SUPPORTS METASTORE: HIVE, UNITY CATALOG
 6. CENTRALIZED DATA GOVERNANCE MODEL
 7. DELTA FORMAT: SUPPORTS ACID, TIME TRAVEL, VERSIONING, SCHEMA EVOLUTION
 8. LAKEHOUSE ARCHITECTURE: DATA LAKE + DELTA
 9. LIVE TABLES: STREAMING, DLT
10. ETL/ELT PIPELINES
11. ORCHESTRATION OF DATA PIPELINES: SCHEDULING WORKFLOWS
12. INTEGRATE NOTEBOOKS AND PIPELINES WITH EXRTERNAL REPOSITORY SYSTEMS LIKE GITHUB
  SPARK
  SPARK IS A GENERAL PURPOSE PROCESSING TOOL/ENGINE/ECOSYSTEM WHICH IS WRITTEN USING FUNCTIONAL PROGRAMMING CALLED "SCALA"

     * SCALA IS A FUNCTIONAL PROGRAMMING LANGUAGE, RUNS IN JVM

  1. SPARK IS A LANGUAGE INDEPENDENT AND PLATFORM INDEPENDENT

       SPARK APIs
       --> CORE SPARK : SCALA
       --> PYSPARK: PYTHON
       --> SPARKLYR: R
       --> SPARK JAVA: JAVA 1.8
       --> SPARK SQL: DATAFRAME (DEFAULT)
       --> SPARKML: MACHINE LEARNING
       --> SPARK STREAMING: REAL TIME DATA ANALYTICS

   sample.txt --> iam learning spark!

   SCALA
   val samplerdd=sc.textFile("sample.txt")
   val uppersample=samplerdd.map(x => x.toUpperCase())
   uppersample.collect()

   PYTHON
   samplerdd=sc.textFile("sample.txt")
   uppersample=samplerdd.map(lambda x:x.upper())
   uppersample.collect()

    SPARK MODE (CLUSTER MANAGER)
     --> LOCAL (SINGLE MACHINE: WINDOWS/LINUX/MAC)
     --> STANDALONE (SPARK OWN CLUSTER)
     --> YARN MODE (SPARK ON HADOOP)
     --> KUBERNETES (DOCKER/CONTAINERS)
     --> MESOS

      * MODE IS DEFINED USING "master" CLAUSE
        EX: pyspark  --master=local[2] or pyspark --master=yarn

   
  2. SPARK IS A DISTRIBUTED MEMORY PROCESSING

  3. SPARK CAN READ/PROCESS ANY TYPE OF DATA
      --> STRUCTURED: CONTAINS SCHEMA -> TABLES; FILES: parquet (binary,columnar,embedded schema) [DEFAULT FOR SPARK/DATABRICKS], AVRO, ORC
      --> SEMI STRUCTURED: CONTAINS FORMAT, NO SCHEMA: csv FILES, tsv FILES, log FILES
      --> UNSTRUCTURED: text FILES, no format no schema

  4. SPARK SUPPORTS SQL SYMANTICS
      --> WE CAN SAVE AS TABLE BY ADDING SCHEMA TO THE METASTORE
      --> WE CAN RUN SQL QUERIES DIRECTLY USING spark.sql

  5. SPARK CAN CONNECT TO REAL TIME SOURCES LIKE KAFKA,SOCKETS,KENISIS

  6. SPARK SUPPORTS SEVERAL IDE's: REPL, SPARK-SUBMIT,JUPYTER,ZEPPLIN,HUE,DATABRICKS NOTEBOOKS,AZURE NOTEBOOKS
        * SPARK SUBMITS THE CODE USING SPARK CONTEXT OBJECT, EARLIER VERSIONS IT WAS "sc", PRESENT IT IS CALLED "spark" [SESSION OBJECT]
        * IN CASE OF REPL, OR DATABRICKS NOTEBOOKS IT IS AUTOMATICALLY CREATED. BUT IN CASE OF OTHER WE HAVE TO EXPLICITLY CREATE USIING: 
                  spark=sparkSession.builder.getOrCreate()




     SPARK ARCHITECTURE
      1. DRIVER: IS THE MAIN CLASS PROGRAM WHICH RUNS THE SPARK JOBS, INTERACTS WITH SPARK CONTEXT, HANDLES TASKS, AND ALSO PROVIDES RESULTS TO THE CLIENT.
      2. TASKS: ACTUAL UNIT OF WORK , MULTIPLE TASKS RUN IN PARALLEL
      3. SUFFLE READS/WRITES: DATA IS TRANSFERRED FOR SUFFULING DURING WIDE TRANSFORMATIONS
      4. RDD: RESLIENT DISTRIBUTED DATASETS
      5. EXECUTOR: JVM/CONTAINER IN WHICH POINT 1 TO 4 HAPPENS/RESIDES


       





















































